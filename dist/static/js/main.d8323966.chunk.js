(this["webpackJsonpreact-mrt"]=this["webpackJsonpreact-mrt"]||[]).push([[0],{100:function(e,a,t){},101:function(e,a,t){e.exports=t.p+"static/media/logo.25bf045c.svg"},102:function(e,a,t){},107:function(e,a){},109:function(e,a){},144:function(e,a){},145:function(e,a){},189:function(e,a,t){},190:function(e,a,t){},191:function(e,a,t){"use strict";t.r(a);var r=t(0),n=t.n(r),i=t(87),p=t.n(i),o=(t(100),t(101),t(102),t(94)),c=t(26),s=t(27),d=t(28),l=t(31),f=t(29),u=t(32),_=t(88),h=t(89),b=t(90),g=t(91),y=t(92),m=t(8),v=t.n(m),E=t(30),x=t.n(E);t(189);function L(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);a&&(r=r.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,r)}return t}function C(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?L(t,!0).forEach((function(a){Object(c.a)(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):L(t).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}var S=function(e){function a(e){var t;return Object(s.a)(this,a),(t=Object(l.a)(this,Object(f.a)(a).call(this,e))).state={displayInteractionTool:!1},t.id=x.a.generate(8),t.state={focusIndex:-1},t}return Object(u.a)(a,e),Object(d.a)(a,[{key:"onEdit",value:function(e,a){this.props.onEdit&&this.props.onEdit(e,a)}},{key:"render",value:function(){var e=this,a=v()(this.props.color).darken(),t=0,r=1.5*this.props.radius,i=0,p=2*this.props.lineHeight,o=this.props.pins.map((function(o,c){return t=i*e.props.lineHeight,n.a.createElement("g",{key:c,transform:"translate(".concat(r,", ").concat(t,")"),onMouseOver:function(a){e.props.isRoot||e.setState(C({},e.state,{focusIndex:c}))},onMouseLeave:function(a){e.props.isRoot||e.setState(C({},e.state,{focusIndex:-1}))}},n.a.createElement("text",{className:"paper-text",id:"text-".concat(e.id,"-").concat(c),fontSize:e.props.fontSize,fill:a,opacity:e.state.focusIndex===c?.3:e.state.focusIndex===c+1?.1:1},o.textPieces.map((function(a,t){return i++,n.a.createElement("tspan",{key:t,x:"0",y:t*e.props.lineHeight},a)}))),n.a.createElement("g",{className:"paper-edit-icon-group",visibility:e.state.focusIndex===c?"none":"hidden",opacity:e.state.focusIndex===c?1:0},n.a.createElement("rect",{x:"0",y:-p,width:5.5*p,height:p,opacity:"0"}),n.a.createElement("g",{className:"paper-edit-icon",opacity:o.edits.rate>0?1:0,visibility:o.edits.rate>0?"none":"hidden"},n.a.createElement(_.a,{x:.5*p,y:-p,fill:"#00a000",width:p,height:p}),n.a.createElement("rect",{x:.5*p,y:-p,width:p,height:p,onClick:function(){return e.onEdit("thumb-delete",o.source)},fill:"transparent"})),n.a.createElement("g",{className:"paper-edit-icon",opacity:o.edits.rate<0?1:0,visibility:o.edits.rate<0?"none":"hidden"},n.a.createElement(h.a,{x:2*p,y:-p,fill:"#a00000",width:p,height:p}),n.a.createElement("rect",{x:2*p,y:-p,width:p,height:p,onClick:function(){return e.onEdit("thumb-delete",o.source)},fill:"transparent"})),n.a.createElement("g",{className:"paper-edit-icon",opacity:o.edits.rate>=0?1:0,visibility:o.edits.rate>=0?"none":"hidden"},n.a.createElement(g.a,{x:2*p,y:-p,fill:"#a00000",width:p,height:p}),n.a.createElement("rect",{x:2*p,y:-p,width:p,height:p,onClick:function(){return e.onEdit("thumb-down",o.source)},fill:"transparent"})),n.a.createElement("g",{className:"paper-edit-icon",opacity:o.edits.rate<=0?1:0,visibility:o.edits.rate<=0?"none":"hidden"},n.a.createElement(b.a,{x:.5*p,y:-p,fill:"#00a000",width:p,height:p}),n.a.createElement("rect",{x:.5*p,y:-p,width:p,height:p,onClick:function(){return e.onEdit("thumb-up",o.source)},fill:"transparent"})),n.a.createElement("g",{className:"paper-edit-icon"},n.a.createElement(y.a,{x:3.5*p,y:-p,fill:"#0000a0",width:p,height:p}),n.a.createElement("rect",{x:3.5*p,y:-p,width:p,height:p,onClick:function(){return e.onEdit("to-exchange",o.source)},fill:"transparent"}))))})),c=t,s="M ".concat(-this.props.radius,",0 A ").concat(this.props.radius," ").concat(this.props.radius," 0 0 1 ").concat(this.props.radius,",0 L ").concat(this.props.radius,",0 A ").concat(this.props.radius," ").concat(this.props.radius," 0 0 1 ").concat(-this.props.radius,",0 L ").concat(-this.props.radius,",0"),d="M ".concat(-this.props.radius,",0 A ").concat(this.props.radius," ").concat(this.props.radius," 0 0 1 ").concat(this.props.radius,",0 L ").concat(this.props.radius,",").concat(c," A ").concat(this.props.radius," ").concat(this.props.radius," 0 0 1 ").concat(-this.props.radius,",").concat(c," L ").concat(-this.props.radius,",0");return n.a.createElement("g",{transform:"translate(".concat(this.props.x,", ").concat(this.props.y,")")},n.a.createElement("g",{onClick:function(){return e.setState(C({},e.state,{displayInteractionTool:!e.state.displayInteractionTool}))}},n.a.createElement("path",{id:"path-".concat(this.id),stroke:this.props.color,strokeWidth:this.props.strokeWidth,fill:"white"}),n.a.createElement("style",null,"\n                        #path-".concat(this.id," {\n                            d: path('").concat(s,"');\n                            transition: d .5s ease;\n                        }\n                        #path-").concat(this.id,":hover {\n                            d: path('").concat(d,"');\n                        }\n                        }"))),o)}}]),a}(n.a.Component),A=t(49),w=t.n(A);t(190);function T(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);a&&(r=r.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,r)}return t}var N=function(e){function a(e){var t;Object(s.a)(this,a),(t=Object(l.a)(this,Object(f.a)(a).call(this,e))).hideSubBranch=t.props.hideSubBranch,t.EraMinRatio=t.props.EraMinRatio||.05,t.lastEraRatio=t.props.lastEraRatio||.2,t.strokeWidth=4,t.labelTextFontSize=64,t.nodeRadius=20,t.nodeTextLeadingMargin=20,t.nodeTextWidth=260,t.nodeTextFontSize=16,t.nodeTextLineHeight=18,t.averageFontWidthRatio=.6,t.nodePaddingLeft=20,t.nodePaddingRight=20,t.nodePaddingTop=32,t.nodePaddingBottom=12,t.nodeOffsetX=t.nodePaddingLeft+t.nodeRadius,t.nodeOffsetY=t.nodePaddingTop+t.nodeRadius,t.nodeWidth=t.nodePaddingLeft+2*t.nodeRadius+t.nodeTextLeadingMargin+t.nodeTextWidth+t.nodePaddingRight,t.nodeTextLines=function(e){return e.pins.reduce((function(e,a){return e+a.textPieces.length}),0)},t.nodeHeight=function(e){return t.nodePaddingTop+t.nodeRadius+Math.max(t.nodeRadius,(e-1)*t.nodeTextLineHeight)+t.nodePaddingBottom},t.nodeTextFold=function(e,a){var r=Math.floor(((a-1)*t.nodeWidth+t.nodeTextWidth)/(t.nodeTextFontSize*t.averageFontWidthRatio));return e.match(new RegExp("([^\\n]{1,".concat(r,"})(\\s|$)"),"g"))},t._data=e.data;var r=function(e){var a=e.paper_id,t=e.paper_year,r=e.paper_venue.trim(),n=e.paper_title.trim(),i=e.paper_citations,p="".concat(t),o=/^(19|20)\d{2}\b/.exec(r);return null==o&&r.length>0?p="".concat(t," ").concat(r):null!=o&&(p="".concat(r)),{id:a,year:t,venue:r,title:n,citations:i,text:"[".concat(p,"] ").concat(n).replace("\t"," ").replace("\n"," ")}};t.data={root:r(t._data.root),branches:[]},t._data.branches.forEach((function(e){t.data.branches.push(e[0].map(r)),t.data.branches.push(e[1].map(r))})),t.data.branches.forEach((function(e){return e.sort((function(e,a){return e.year===a.year?a.citations-e.citations:a.year-e.year}))})),t.clusterNames=t._data.branches.map((function(e,a){return"Cluster ".concat(a)}));var n={};return n[t.data.root.id]={rate:0,cluster:0},t.data.branches.forEach((function(e,a){return e.forEach((function(e){return n[e.id]={rate:0,cluster:Math.floor(a/2)}}))})),t.state={userEdits:n,toExchange:null},t}return Object(u.a)(a,e),Object(d.a)(a,[{key:"render",value:function(){var e=this,a={colorDefs:[],nodes:{},edges:[]},t={root:w.a.cloneDeep(this.data.root),branches:this.data.branches.map((function(){return[]}))};this.data.branches.forEach((function(a,r){return a.forEach((function(a){var n=w.a.cloneDeep(a);t.branches[2*e.state.userEdits[a.id].cluster+r%2].push(n)}))})),this.hideSubBranch&&(t.branches=t.branches.map((function(e,a){return a%2===0?e:[]})));for(var r=t.branches.length,i=Math.floor(r/2),p=v.a.scale()(.5),s=v.a.cubehelix().start(200).rotations(3).gamma(.7).lightness([.2,.6]).scale().correctLightness().colors(i),d=[],l=t.branches.flat().map((function(e){return e.year})).sort((function(e,a){return a-e})),f=l[0],u=1,_=this.EraMinRatio*l.length,h=this.lastEraRatio*l.length,b=1;b<l.length;b++)l[b]===l[b-1]||u<_||b>l.length-h?u+=1:(d.push({from:l[b-1],to:f,cnt:u}),f=l[b],u=1);d.push({from:l[l.length-1],to:f,cnt:u}),a.generateGradientColor=function(e,t,r,i,p,o){var c=x.a.generate(8);return a.colorDefs.push(n.a.createElement("defs",{key:c},n.a.createElement("linearGradient",{id:c,x1:r,y1:i,x2:p,y2:o,gradientUnits:"userSpaceOnUse"},n.a.createElement("stop",{offset:"20%",stopColor:e}),n.a.createElement("stop",{offset:"80%",stopColor:t})))),"url('#".concat(c,"')")},a.nodes.root={isRoot:!0,x:this.nodeWidth*(t.branches.length-1)/2+this.nodeOffsetX,y:this.nodeOffsetY,color:p,pins:[{source:t.root,edits:this.state.userEdits[t.root.id],textPieces:this.nodeTextFold(t.root.text,2)}],clusterIndex:-1},a.nodes.branches=t.branches.map((function(a,t){return d.map((function(r,n){return{isRoot:!1,x:e.nodeWidth*t+e.nodeOffsetX,y:0,color:v()(s[Math.floor(t/2)]).brighten(t%2),pins:a.filter((function(e){return e.year>=r.from&&e.year<=r.to})).map((function(a){return{source:a,edits:e.state.userEdits[a.id]}})).sort((function(e,a){return e.source.year===a.source.year?a.source.citations-e.source.citations:a.source.year-e.source.year})),era:n,clusterIndex:Math.floor(t/2)}}))})),a.nodes.branches.forEach((function(t,n){return t.forEach((function(t,i){if(0!==t.pins.length){var p=n<r-1&&0===a.nodes.branches[n+1][i].pins.length?2:1;t.pins.forEach((function(a){return a.textPieces=e.nodeTextFold(a.source.text,p)}))}}))}));var g=this.nodeHeight(this.nodeTextLines(a.nodes.root))+this.nodePaddingTop,y=g+this.nodePaddingBottom;d.forEach((function(t,r){var n=a.nodes.branches.reduce((function(a,t){return Math.max(a,e.nodeTextLines(t[r]))}),0);a.nodes.branches.forEach((function(a){return a[r].y=y+e.nodeOffsetY})),y+=e.nodeHeight(n)}));var m=a.nodes.root;a.edges.push({x1:m.x,y1:m.y,x2:m.x,y2:g,color:p});var E=a.nodes.branches[0][0],L=a.nodes.branches[r-2][0];a.edges.push({x1:E.x,y1:g,x2:L.x,y2:g,color:p}),a.nodes.branches.forEach((function(t,r){var n=t.filter((function(e){return e.pins.length>0}));if(0!==n.length){var i=r%2===0?0:n[0].era,o=n[n.length-1].era;if(r%2===0){var c=a.nodes.branches[r+1].filter((function(e){return e.pins.length>0}));c.length>0&&(o=Math.max(o,c[0].era))}for(var s=i+1;s<=o;s++){var d=t[s],l=r>0?a.nodes.branches[r-1][s]:null,f=0===d.pins.length&&(r>0&&l.pins.length>0||s===o)?d.y-e.nodeRadius-e.nodeTextLineHeight:d.y;d=t[s-1],l=r>0?a.nodes.branches[r-1][s-1]:null;var u=0===d.pins.length&&r>0&&l.pins.length>0?d.y-e.nodeOffsetY+e.nodeHeight(e.nodeTextLines(l))-e.nodePaddingBottom+e.nodeTextLineHeight:d.y;a.edges.push({x1:d.x,y1:f,x2:d.x,y2:u,color:d.color})}if(r%2===0){var _=t[0],h=r>0?a.nodes.branches[r-1][0]:null,b=0===_.pins.length&&r>0&&h.pins.length>0?_.y-e.nodeRadius-e.nodeTextLineHeight:_.y;a.edges.push({x1:_.x,y1:g,x2:_.x,y2:b,color:a.generateGradientColor(p,_.color,_.x,g,_.x,b)})}else{var y=t[i],m=a.nodes.branches[r-1][i],v=y.y-e.nodeRadius-e.nodeTextLineHeight,E=y.y;a.edges.push({x1:y.x,y1:E,x2:y.x,y2:v,color:y.color}),a.edges.push({x1:y.x,y1:v,x2:m.x,y2:v,color:a.generateGradientColor(y.color,m.color,y.x,v,m.x,v)})}}})),y+=3*this.labelTextFontSize;var C=function(a,t,r){var n=function(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?T(t,!0).forEach((function(a){Object(c.a)(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):T(t).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}({},e.state);"thumb-up"===a&&n.userEdits[t.id].rate<=0?(n.userEdits[t.id].rate=1,e.setState(n)):"thumb-down"===a&&n.userEdits[t.id].rate>=0?(n.userEdits[t.id].rate=-1,e.setState(n)):"thumb-delete"===a&&0!==n.userEdits[t.id].rate?(n.userEdits[t.id].rate=0,e.setState(n)):"to-exchange"===a&&null===n.toExchange?(n.toExchange=t,e.setState(n)):"exchange"===a&&(n.userEdits[t.id].cluster=r,n.toExchange=null,e.setState(n))},A=this.nodeWidth*t.branches.length;return n.a.createElement("svg",{className:"mrt",width:"100%",viewBox:"0 0 ".concat(A," ").concat(y)},a.colorDefs,n.a.createElement("g",{className:"mrt-background"},n.a.createElement("rect",{x:"0",y:"0",width:A,height:g,fill:v()(p).luminance(.9)})),a.nodes.branches.map((function(a,t){if(t%2===0)return n.a.createElement("g",{className:"mrt-background",key:t,opacity:null===e.state.toExchange?1:0},n.a.createElement("rect",{x:e.nodeWidth*t,y:g,width:2*e.nodeWidth,height:y-g,fill:v()(a[0].color).luminance(.9)}),n.a.createElement("text",{x:e.nodeWidth*t+e.nodeOffsetX,y:y-e.labelTextFontSize,fill:v()(a[0].color).luminance(.7),fontSize:e.labelTextFontSize},e.clusterNames[Math.floor(t/2)]))})),a.edges.map((function(a,t){return n.a.createElement("line",{key:t,x1:a.x1,y1:a.y1,x2:a.x2,y2:a.y2,strokeWidth:e.strokeWidth-1,stroke:a.color})})),[a.nodes.root].concat(Object(o.a)(a.nodes.branches.flat(1/0))).map((function(a,t){return a.pins.length>0&&n.a.createElement(S,{key:t,pins:a.pins,x:a.x,y:a.y,radius:e.nodeRadius,lineHeight:e.nodeTextLineHeight,color:a.color,isRoot:a.isRoot,clusterIndex:a.clusterIndex,clusterNames:e.clusterNames,fontSize:e.nodeTextFontSize,strokeWidth:e.strokeWidth,onEdit:C})})),a.nodes.branches.map((function(a,t){if(t%2===0){var r=null!==e.state.toExchange&&Math.floor(t/2)===e.state.userEdits[e.state.toExchange.id].cluster;return n.a.createElement("g",{className:"mrt-background",key:t,opacity:null===e.state.toExchange?0:1,visibility:null===e.state.toExchange?"hidden":"none",onClick:function(){return C("exchange",e.state.toExchange,Math.floor(t/2))}},n.a.createElement("rect",{className:"mrt-background-card",x:e.nodeWidth*t,y:g,width:2*e.nodeWidth,height:y-g,fill:v()(a[0].color).luminance(.5)}),n.a.createElement("text",{className:"mrt-background-text",x:e.nodeWidth*t+e.nodeOffsetX,y:y-e.labelTextFontSize,fill:v()(a[0].color).luminance(.2),fontSize:e.labelTextFontSize*(r?1:.5)},e.clusterNames[Math.floor(t/2)]))}})))}}]),a}(n.a.Component),R=t(93);var I=function(){return n.a.createElement("div",{className:"App"},n.a.createElement(N,{data:R}))};Boolean("localhost"===window.location.hostname||"[::1]"===window.location.hostname||window.location.hostname.match(/^127(?:\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}$/));p.a.render(n.a.createElement(I,null),document.getElementById("root")),"serviceWorker"in navigator&&navigator.serviceWorker.ready.then((function(e){e.unregister()}))},93:function(e){e.exports=JSON.parse('{"root":{"paper_id":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","paper_title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","paper_year":2018,"paper_venue":"NAACL-HLT","paper_citations":997},"branches":[[[{"paper_id":"9fa8d73e572c3ca824a04a5f551b602a17831bc5","paper_title":"Domain Adaptation with Structural Correspondence Learning","paper_year":2006,"paper_venue":"EMNLP","paper_citations":925},{"paper_id":"2c5135a0531bc5ad7dd890f018e67a40529f5bcb","paper_title":"A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data","paper_year":2005,"paper_venue":"J. Mach. Learn. Res.","paper_citations":822},{"paper_id":"10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb","paper_title":"Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition","paper_year":2003,"paper_venue":"CoNLL","paper_citations":789},{"paper_id":"128cb6b891aee1b5df099acb48e2efecfcff689f","paper_title":"The Winograd Schema Challenge","paper_year":2011,"paper_venue":"AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning","paper_citations":198},{"paper_id":"0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38","paper_title":"Semi-supervised sequence tagging with bidirectional language models","paper_year":2017,"paper_venue":"ACL","paper_citations":145},{"paper_id":"421fc2556836a6b441de806d7b393a35b6eaea58","paper_title":"Contextual String Embeddings for Sequence Labeling","paper_year":2018,"paper_venue":"COLING","paper_citations":85},{"paper_id":"cb0f3ee1e98faf92429d601cdcd76c69c1e484eb","paper_title":"Neural Network Acceptability Judgments","paper_year":2018,"paper_venue":"ArXiv","paper_citations":34},{"paper_id":"0c47cad9729c38d9db1f75491b1ee4bd883a5d4e","paper_title":"Semi-Supervised Sequence Modeling with Cross-View Training","paper_year":2018,"paper_venue":"EMNLP","paper_citations":32}],[{"paper_id":"4f3db7f0170107a391b34d1bc63204cb7c4f07ba","paper_title":"Surveys in Combinatorics, 1989: On the method of bounded differences","paper_year":1989,"paper_venue":"","paper_citations":999},{"paper_id":"65d73171eb6d7c7121afa9a3ea944b2108940772","paper_title":"Applied Logistic Regression Analysis.","paper_year":1996,"paper_venue":"","paper_citations":999},{"paper_id":"8213dbed4db44e113af3ed17d6dad57471a0c048","paper_title":"The Nature of Statistical Learning Theory","paper_year":1995,"paper_venue":"Statistics for Engineering and Information Science","paper_citations":999},{"paper_id":"17accbdd4aa3f9fad6af322bc3d7f4d5b648d9cd","paper_title":"Transductive Inference for Text Classification using Support Vector Machines","paper_year":1999,"paper_venue":"ICML","paper_citations":999},{"paper_id":"b25663fa149be5286de193c13324098aedd7e2cc","paper_title":"Opinion Mining and Sentiment Analysis","paper_year":2007,"paper_venue":"Foundations and Trends in Information Retrieval","paper_citations":999},{"paper_id":"dd90dee12840f4e700d8146fb111dbc863a938ad","paper_title":"Reinforcement Learning: An Introduction","paper_year":1988,"paper_venue":"IEEE Transactions on Neural Networks","paper_citations":999},{"paper_id":"02485a373142312c354b79552b3d326913eaf86d","paper_title":"Semi-Supervised Learning Using Gaussian Fields and Harmonic Functions","paper_year":2003,"paper_venue":"ICML","paper_citations":999},{"paper_id":"486af640c427afba9036799cbe2bc41774a4d6c2","paper_title":"Constraints on variables in syntax","paper_year":1967,"paper_venue":"","paper_citations":999},{"paper_id":"dc8b25e35a3acb812beb499844734081722319b4","paper_title":"The FERET database and evaluation procedure for face-recognition algorithms","paper_year":1998,"paper_venue":"Image Vision Comput.","paper_citations":999},{"paper_id":"f28cd8803a0d453d389cdc270923231cbf4ebafc","paper_title":"Computing Machinery and Intelligence","paper_year":1950,"paper_venue":"","paper_citations":999}]],[[{"paper_id":"05dd7254b632376973f3a1b4d39485da17814df5","paper_title":"SQuAD: 100, 000+ Questions for Machine Comprehension of Text","paper_year":2016,"paper_venue":"EMNLP","paper_citations":926},{"paper_id":"766ce989b8b8b984f7a4691fd8c9af4bdb2b74cd","paper_title":"\\"Cloze procedure\\": a new tool for measuring readability.","paper_year":1953,"paper_venue":"","paper_citations":698},{"paper_id":"007ab5528b3bd310a80d553cccad4b78dc496b02","paper_title":"Bidirectional Attention Flow for Machine Comprehension","paper_year":2016,"paper_venue":"ICLR","paper_citations":611},{"paper_id":"f010affab57b5fcf1cd6be23df79d8ec98c7289c","paper_title":"TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension","paper_year":2017,"paper_venue":"ACL","paper_citations":214},{"paper_id":"8c1b00128e74f1cd92aede3959690615695d5101","paper_title":"QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension","paper_year":2018,"paper_venue":"ICLR","paper_citations":164},{"paper_id":"3c78c6df5eb1695b6a399e346dde880af27d1016","paper_title":"Simple and Effective Multi-Paragraph Reading Comprehension","paper_year":2017,"paper_venue":"ACL","paper_citations":96},{"paper_id":"e0222a1ae6874f7fff128c3da8769ab95963da04","paper_title":"Reinforced Mnemonic Reader for Machine Reading Comprehension","paper_year":2017,"paper_venue":"IJCAI","paper_citations":46},{"paper_id":"26b47e35fe6e4260fdf7b7cc98f279a73c277494","paper_title":"Multi-granularity hierarchical attention fusion networks for reading comprehension and question answering","paper_year":2018,"paper_venue":"ACL","paper_citations":35},{"paper_id":"27e98e09cf09bc13c913d01676e5f32624011050","paper_title":"U-Net: Machine Reading Comprehension with Unanswerable Questions","paper_year":2018,"paper_venue":"ArXiv","paper_citations":8}],[]],[[{"paper_id":"38211dc39e41273c0007889202c69f841e02248a","paper_title":"ImageNet: A large-scale hierarchical image database","paper_year":2009,"paper_venue":"2009 IEEE Conference on Computer Vision and Pattern Recognition","paper_citations":999},{"paper_id":"081651b38ff7533550a3adfc1c00da333a8fe86c","paper_title":"How transferable are features in deep neural networks?","paper_year":2014,"paper_venue":"NIPS","paper_citations":997},{"paper_id":"0e6824e137847be0599bb0032e37042ed2ef5045","paper_title":"Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books","paper_year":2015,"paper_venue":"2015 IEEE International Conference on Computer Vision (ICCV)","paper_citations":287},{"paper_id":"af5c4b80fbf847f69a202ba5a780a3dd18c1a027","paper_title":"SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference","paper_year":2018,"paper_venue":"EMNLP","paper_citations":64}],[{"paper_id":"1a2a770d23b4a171fa81de62a78a3deb0588f238","paper_title":"Visualizing and Understanding Convolutional Networks","paper_year":2013,"paper_venue":"ECCV","paper_citations":999},{"paper_id":"0626908dd710b91aece1a81f4ca0635f23fc47f3","paper_title":"Rethinking the Inception Architecture for Computer Vision","paper_year":2015,"paper_venue":"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","paper_citations":999},{"paper_id":"f6fffe049408c7b0343a1bdcefe0dc3d0256646d","paper_title":"Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories","paper_year":2004,"paper_venue":"2004 Conference on Computer Vision and Pattern Recognition Workshop","paper_citations":999},{"paper_id":"446fbff6a2a7c9989b0a0465f960e236d9a5e886","paper_title":"Context Encoders: Feature Learning by Inpainting","paper_year":2016,"paper_venue":"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","paper_citations":999},{"paper_id":"1d827e24143e5fdfe709d33b7b13a9a24d402efd","paper_title":"Learning Deep Features for Scene Recognition using Places Database","paper_year":2014,"paper_venue":"NIPS","paper_citations":999},{"paper_id":"13313124277b42dc61096edd170e74b87592ddab","paper_title":"Image Denoising Via Sparse and Redundant Representations Over Learned Dictionaries","paper_year":2006,"paper_venue":"IEEE Transactions on Image Processing","paper_citations":999},{"paper_id":"14318685b5959b51d0f1e3db34643eb2855dc6d9","paper_title":"Going deeper with convolutions","paper_year":2014,"paper_venue":"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","paper_citations":999},{"paper_id":"abd1c342495432171beb7ca8fd9551ef13cbd0ff","paper_title":"ImageNet Classification with Deep Convolutional Neural Networks","paper_year":2012,"paper_venue":"NIPS","paper_citations":999},{"paper_id":"1109b663453e78a59e4f66446d71720ac58cec25","paper_title":"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks","paper_year":2013,"paper_venue":"ICLR","paper_citations":999},{"paper_id":"1e80f755bcbf10479afd2338cec05211fdbd325c","paper_title":"Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations","paper_year":2009,"paper_venue":"ICML","paper_citations":999},{"paper_id":"784da2a7b53a16d2243f747e14946cc5e3476af0","paper_title":"VQA: Visual Question Answering","paper_year":2015,"paper_venue":"ICCV","paper_citations":999},{"paper_id":"4b53c6b0193935e710e15a0e1ec3f9f25502e450","paper_title":"Xception: Deep Learning with Depthwise Separable Convolutions","paper_year":2016,"paper_venue":"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","paper_citations":999},{"paper_id":"14ce7635ff18318e7094417d0f92acbec6669f1c","paper_title":"DeepFace: Closing the Gap to Human-Level Performance in Face Verification","paper_year":2014,"paper_venue":"2014 IEEE Conference on Computer Vision and Pattern Recognition","paper_citations":999}]],[[{"paper_id":"843959ffdccf31c6694d135fad07425924f785b1","paper_title":"Extracting and composing robust features with denoising autoencoders","paper_year":2008,"paper_venue":"ICML","paper_citations":999},{"paper_id":"3febb2bed8865945e7fddc99efd791887bb7e14f","paper_title":"Deep contextualized word representations","paper_year":2018,"paper_venue":"NAACL-HLT","paper_citations":999},{"paper_id":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","paper_title":"Attention Is All You Need","paper_year":2017,"paper_venue":"NIPS","paper_citations":997},{"paper_id":"6e795c6e9916174ae12349f5dc3f516570c17ce8","paper_title":"Skip-Thought Vectors","paper_year":2015,"paper_venue":"NIPS","paper_citations":846},{"paper_id":"5d833331b0e22ff359db05c62a8bca18c4f04b68","paper_title":"One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling","paper_year":2013,"paper_venue":"INTERSPEECH","paper_citations":423},{"paper_id":"4aa9f5150b46320f534de4747a2dd0cd7f3fe292","paper_title":"Semi-supervised Sequence Learning","paper_year":2015,"paper_venue":"NIPS","paper_citations":364},{"paper_id":"1e077413b25c4d34945cc2707e17e46ed4fe784a","paper_title":"Universal Language Model Fine-tuning for Text Classification","paper_year":2018,"paper_venue":"ACL","paper_citations":332},{"paper_id":"bc8fa64625d9189f5801837e7b133e7fe3c581f7","paper_title":"Learned in Translation: Contextualized Word Vectors","paper_year":2017,"paper_venue":"NIPS","paper_citations":244},{"paper_id":"7f4afc1bf3272ae6ec00b46e27efc4a4f6b0826d","paper_title":"MaskGAN: Better Text Generation via Filling in the _______","paper_year":2018,"paper_venue":"ICLR","paper_citations":95},{"paper_id":"bc1d609520290e0460c49b685675eb5a57fa5935","paper_title":"An efficient framework for learning sentence representations","paper_year":2018,"paper_venue":"ICLR","paper_citations":57},{"paper_id":"4361e64f2d12d63476fdc88faf72a0f70d9a2ffb","paper_title":"Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units","paper_year":2017,"paper_venue":"ArXiv","paper_citations":45},{"paper_id":"ac11062f1f368d97f4c826c317bf50dcc13fdb59","paper_title":"Dissecting Contextual Word Embeddings: Architecture and Representation","paper_year":2018,"paper_venue":"EMNLP","paper_citations":43},{"paper_id":"a97dc52807d80454e78d255f9fbd7b0fab56bd03","paper_title":"Discourse-Based Objectives for Fast Unsupervised Sentence Representation Learning","paper_year":2017,"paper_venue":"ArXiv","paper_citations":35},{"paper_id":"b9de9599d7241459db9213b5cdd7059696f5ef8d","paper_title":"Character-Level Language Modeling with Deeper Self-Attention","paper_year":2018,"paper_venue":"AAAI","paper_citations":26}],[{"paper_id":"0a7fb47217e6d0e3b80159bc4f9e02a50ea1f391","paper_title":"Seeing Stars: Exploiting Class Relationships for Sentiment Categorization with Respect to Rating Scales","paper_year":2005,"paper_venue":"ACL","paper_citations":999},{"paper_id":"83174a52f38c80427e237446ccda79e2a9170742","paper_title":"Deep Sparse Rectifier Neural Networks","paper_year":2011,"paper_venue":"AISTATS","paper_citations":999},{"paper_id":"74fc396d0b8ec548d600395182f12c9b06cc84e9","paper_title":"Distilling the Knowledge in a Neural Network","paper_year":2015,"paper_venue":"ArXiv","paper_citations":999},{"paper_id":"d7da009f457917aa381619facfa5ffae9329a6e9","paper_title":"Bleu: a Method for Automatic Evaluation of Machine Translation","paper_year":2001,"paper_venue":"ACL","paper_citations":999},{"paper_id":"052b1d8ce63b07fec3de9dbb583772d860b7c769","paper_title":"Learning representations by back-propagating errors","paper_year":1986,"paper_venue":"Nature","paper_citations":999},{"paper_id":"bff427c18caa092afff57a400e353fde79254f22","paper_title":"BACKPROPAGATION THROUGH TIME: WHAT IT DOES AND HOW TO DO IT","paper_year":1990,"paper_venue":"","paper_citations":999},{"paper_id":"dbde7dfa6cae81df8ac19ef500c42db96c3d1edd","paper_title":"Google\'s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation","paper_year":2016,"paper_venue":"ArXiv","paper_citations":999},{"paper_id":"5c3785bc4dc07d7e77deef7e90973bdeeea760a5","paper_title":"TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems","paper_year":2015,"paper_venue":"ArXiv","paper_citations":999},{"paper_id":"6c8b30f63f265c32e26d999aa1fef5286b8308ad","paper_title":"Dropout: a simple way to prevent neural networks from overfitting","paper_year":2014,"paper_venue":"J. Mach. Learn. Res.","paper_citations":999},{"paper_id":"39dba6f22d72853561a4ed684be265e179a39e4f","paper_title":"Sequence to Sequence Learning with Neural Networks","paper_year":2014,"paper_venue":"NIPS","paper_citations":999},{"paper_id":"0b544dfe355a5070b60986319a3f51fb45d1348e","paper_title":"Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation","paper_year":2014,"paper_venue":"EMNLP","paper_citations":999},{"paper_id":"0d67362a5630ec3b7562327acc278c1c996454b5","paper_title":"Learning Deep Architectures for AI","paper_year":2007,"paper_venue":"Foundations and Trends in Machine Learning","paper_citations":999},{"paper_id":"1cff7cc15555c38607016aaba24059e76b160adb","paper_title":"Annotating Expressions of Opinions and Emotions in Language","paper_year":2005,"paper_venue":"Language Resources and Evaluation","paper_citations":999},{"paper_id":"2f83f6e1afadf0963153974968af6b8342775d82","paper_title":"Framewise phoneme classification with bidirectional LSTM and other neural network architectures","paper_year":2005,"paper_venue":"Neural Networks","paper_citations":999},{"paper_id":"0b8759d61e93b809df16d9fe9010d2a2d7241c74","paper_title":"Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)","paper_year":2015,"paper_venue":"ICLR","paper_citations":999},{"paper_id":"93499a7c7f699b6630a86fad964536f9423bb6d0","paper_title":"Effective Approaches to Attention-based Neural Machine Translation","paper_year":2015,"paper_venue":"EMNLP","paper_citations":999},{"paper_id":"cdcf7cb29f37ac0546961ea8a076075b9cc1f992","paper_title":"Mining and summarizing customer reviews","paper_year":2004,"paper_venue":"KDD","paper_citations":999},{"paper_id":"146f6f6ed688c905fb6e346ad02332efd5464616","paper_title":"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention","paper_year":2015,"paper_venue":"ICML","paper_citations":999}]],[[{"paper_id":"1510cf4b8abea80b9f352325ca4c132887de21a0","paper_title":"Distributed Representations of Sentences and Documents","paper_year":2014,"paper_venue":"ICML","paper_citations":999},{"paper_id":"687bac2d3320083eb4530bf18bb8f8f721477600","paper_title":"Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank","paper_year":2013,"paper_venue":"EMNLP","paper_citations":999},{"paper_id":"1a07186bc10592f0330655519ad91652125cd907","paper_title":"A unified architecture for natural language processing: deep neural networks with multitask learning","paper_year":2008,"paper_venue":"ICML","paper_citations":999},{"paper_id":"f37e1b62a767a307c046404ca96bc140b3e68cb5","paper_title":"Glove: Global Vectors for Word Representation","paper_year":2014,"paper_venue":"EMNLP","paper_citations":999},{"paper_id":"87f40e6f3022adbc1f1905e3e506abad05a9964f","paper_title":"Distributed Representations of Words and Phrases and their Compositionality","paper_year":2013,"paper_venue":"NIPS","paper_citations":999},{"paper_id":"783480acff435bfbc15ffcdb4f15eccddaa0c810","paper_title":"Class-Based n-gram Models of Natural Language","paper_year":1992,"paper_venue":"Computational Linguistics","paper_citations":999},{"paper_id":"dac72f2c509aee67524d3321f77e97e8eff51de6","paper_title":"Word Representations: A Simple and General Method for Semi-Supervised Learning","paper_year":2010,"paper_venue":"ACL","paper_citations":999},{"paper_id":"1005645c05585c2042e3410daeed638b55e2474d","paper_title":"A Scalable Hierarchical Distributed Language Model","paper_year":2008,"paper_venue":"NIPS","paper_citations":591},{"paper_id":"26e743d5bd465f49b9538deaf116c15e61b7951f","paper_title":"Learning Distributed Representations of Sentences from Unlabelled Data","paper_year":2016,"paper_venue":"HLT-NAACL","paper_citations":238},{"paper_id":"59761abc736397539bdd01ad7f9d91c8607c0457","paper_title":"context2vec: Learning Generic Context Embedding with Bidirectional LSTM","paper_year":2016,"paper_venue":"CoNLL","paper_citations":124}],[{"paper_id":"d4e8bed3b50a035e1eabad614fe4218a34b3b178","paper_title":"An Empirical Study of Smoothing Techniques for Language Modeling","paper_year":1996,"paper_venue":"ACL","paper_citations":999},{"paper_id":"0a6383b13794452fb7339a7f8a5384885186ccf6","paper_title":"Enriching Word Vectors with Subword Information","paper_year":2016,"paper_venue":"Transactions of the Association for Computational Linguistics","paper_citations":999},{"paper_id":"c7093913b4daa0f34d9d58a41ceb0475cc3cc9f4","paper_title":"Producing high-dimensional semantic spaces from lexical co-occurrence","paper_year":1996,"paper_venue":"","paper_citations":999}]],[[{"paper_id":"f04df4e20a18358ea2f689b4c129781628ef7fc1","paper_title":"A large annotated corpus for learning natural language inference","paper_year":2015,"paper_venue":"EMNLP","paper_citations":808},{"paper_id":"ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c","paper_title":"Supervised Learning of Universal Sentence Representations from Natural Language Inference Data","paper_year":2017,"paper_venue":"EMNLP","paper_citations":474},{"paper_id":"2cd8e8f510c89c7c18268e8ad51c061e459ad321","paper_title":"A Decomposable Attention Model for Natural Language Inference","paper_year":2016,"paper_venue":"EMNLP","paper_citations":410},{"paper_id":"5ded2b8c64491b4a67f6d39ce473d4b9347a672e","paper_title":"A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference","paper_year":2017,"paper_venue":"NAACL-HLT","paper_citations":314},{"paper_id":"93b8da28d006415866bf48f9a6e06b5242129195","paper_title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding","paper_year":2018,"paper_venue":"ICLR","paper_citations":159},{"paper_id":"a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096","paper_title":"SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation","paper_year":2017,"paper_venue":"SemEval@ACL","paper_citations":156}],[{"paper_id":"68c03788224000794d5491ab459be0b2a2c38677","paper_title":"WORDNET: A Lexical Database for English","paper_year":1992,"paper_venue":"HLT","paper_citations":999},{"paper_id":"24ff26b9cf71d9f9a8e7dedf0cfe56d105363cd3","paper_title":"The Berkeley FrameNet Project","paper_year":1998,"paper_venue":"COLING-ACL","paper_citations":999},{"paper_id":"3d07b5087e53c6f7c228b3c7e769494527be228e","paper_title":"A Study of Translation Edit Rate with Targeted Human Annotation","paper_year":2006,"paper_venue":"","paper_citations":999}]]]}')},95:function(e,a,t){e.exports=t(191)}},[[95,1,2]]]);
//# sourceMappingURL=main.d8323966.chunk.js.map